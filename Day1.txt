### 스키너의 강화 연구
1. 굶긴 쥐를 상자에 넣는다.
2. 쥐는 돌아다니다가 우연히 상자 안에 있는 지렛대를 누르게 된다
3. 지렛대를 누르자 먹이가 나온다.
4. 지렛대를 누르는 행동과 먹이와의 상관관계를 모르는 쥐는 다시 돌아다닌다.
5. 그러다가 우연히 쥐가 다시 지렛대를 누르면 쥐는 이제 먹이와 지렛대 사이의 관계를 알게 되고 점점 지렛대를 자주 누르게 된다.
6. 이 과정을 반복하면서 쥐는 지렛대를 누르면 먹이를 먹을 수 있다는 것을 학습한다.


### 우리 주변에서의 강화
아이가 첫걸음을 떼는 과정도 일종의 강화라고 할 수 있다.
1. 아이는 걷는 것을 배운 적이 없다.
2. 아이는 스스로 이것저것 시도해 보다가 우연히 걷게 된다.
3. 자신이 하는 행동과 걷게 된다는 보상 사이의 상관관계를 모르는 아이는 다시 넘어진다.
4. 시간이 지남에 따라 그 관계를 학습해서 잘 걷게 된다.


## 강화학습
ㆍ 에이전트는 사전 지식이 없는 상태에서 학습함
ㆍ 에이전트는 자신이 놓인 환경에서 자신의 상태를 인식한 후 행동
ㆍ 환경은 에이전트에게 보상을 주고 다음 상태를 알려줌
ㆍ 에이전트는 보상을 통해 어떤 행동이 좋은 행동인지 간접적으로 알게 됨


### 강화학습 문제
결정을 순차적으로 내려야 하는 문제에 강화학습을 적용한다.
이 문제를 풀기 위해서는 문제를 수학적으로 정의해야 한다.

1. 상태 (State)
    현재 에이전트의 정보 ( 정적인 요소 + 동적인 요소 )
2. 행동 (Action)
    에이전트가 어떠한 상태에서 취할 수 있는 행동
3. 보상 (Reward)
    에이전트가 학습할 수 있는 유일한 정보, 자신이 했던 행동을 평가할 수 있는 지표
    강화학습의 목표는 시간에 따라 얻는 보상의 합을 최대로 하는 정책을 찾는 것
4. 정책 (Policy)
    순차적 행동 결정 문제에서 구해야 할 답
    모든 상태에 대해 에이전트가 어떤 행동을 해야 하는지 정해놓은 것


## MDP
강화 학습은 순차적으로 행동을 계속 결정해야 하는 문제를 푸는 것
→ 이 문제를 수학적으로 표현한 것이 MDP(Markov Decision Process)

ㆍ MDP의 구성 요소
    ㆍ 상태
    ㆍ 행동
    ㆍ 보상 함수
    ㆍ 상태 변환 확률 (State Transition probability)
    ㆍ 감가율 (Discount Factor)

### 그리드 월드
격자로 이뤄진 환경에서 문제를 푸는 각종 예제

## 상태
에이전트가 관찰 가능한 상태의 집합 : S
ㆍ 그리드 월드에서 상태의 개수는 유한
ㆍ 그리드 월드에 상태가 5개 있을 경우, 수식으로 표현하면
    S = {(x1, y1), (x2, y2), (x3, y3), (x4, y4), (x5, y5)}
ㆍ 그리드 월드에서 상태는 격자 상의 각 위치 ( 좌표 )
ㆍ 에이전트는 시간에 따라 상태 집합 안에 있는 상태를 탐험한다.
    이 때 시간을 t, 시간 t일 떄의 상태를 St라고 표현한다.
ㆍ 예를 들어, 시간이 t일 때 상태가 (1,3)이라면 St = (1,3)

에이전트가 관찰 가능한 상태의 집합 : S
ㆍ 어떤 t에서의 상태 St는 정해진 것이 아니다.
ㆍ 때에 따라서 t = 1일 때 S_t = (1,3)일 수도 있고 St = (4,2)일 수도 있다.

'S_t = s'
→ "시간 t에서의 상태 S_t가 어떤 상태 s다."


## 행동

에이전트가 상태 St에서 할 수 있는 가능한 행동의 집합 : A

ㆍ 보통 에이전트가 할 수있는 행동은 모든 상태에서같다.   At = a
ㆍ "시간 t에 에이전트가 특정한 행동 a를했다."
ㆍ t라는 시간에 에이전트가 어떤 행동을 할 지는 정해져 있지않으므로 At처럼 대문자로 표현한다.
ㆍ 그리드 월드에서 에이전트가 할 수 있는 행동은 A = {up, down, left, right}
ㆍ 만약 시간 t에서 상태가 (1,3)이고 At = right라면 다음 시간의 상태는 (2,3)이 된다.

## 보상함수

에이전트가 학습할 수 있는 유일한정보
ㆍ보상 함수 (Reward Function)
    R_s^a = E[R_t+1|S_t = s,A_t = a]

ㆍ 시간 t일떄 상태가 S_t = s 이고 그 상태에서 행동이 A_t = a를 했을 경우
   받을 보상에 대한 기댓값(Expectation) E
ㆍ 에이전트가 어떤 상태에서 행동한 시간 : t
   보상을 받는 시간 : t + 1

ㆍ 이유 : 에이전트가 보상을 알고 있는게 아니라 환경이 알려주기 때문
   에이전트가 상태 s에서 행동 a를 하면 환경은 에이전트가 가게 되는 다음 상태 + s'와
   에이전트가 받을 보상을 에이전트에게 알려준다. 이 시점이 t + 1이다.


## 상태 변환 확률

에이전트가 어떤 상태에서 어떤 행동을 취하면 상태가 변한다.
하지만 어떤 이유로 인해 다음 상태로 변하지 못할 수도 있다.

→ 상태의 변화에는 확률적인 요인이 들어간다.
이를 수치적으로 표현한 것이 상태 변환 확률.

P_ss'^a = P[S_t+1 = s'|S_t = s,A_t = a]


## 감가율

에이전트는 항상 현재시점에서 판단을 내리기 때문에
현재에 가까운 보상일수록 더 큰 가치를 갖는다.
보상의 크기가 100일 때, 에이전트가 현재 시각에 보상을 받을 때는
100의 크기 그대로 받아들이지만 현재로부터 일정 시간이 지나서
보상을 받으면 크기가 100이라고 생각하지 않는다.
에이전트는 그 보상을 얼마나 시간이 지나서 받는지를 고려해
감가시켜 현재의 가치로 따진다.

우리는 이자를 통해 나중에 받을 보상에 추가적인 보상을 더해 현재의 보상과 같게 만든다.
반대로 말하면 같은 보상이면 나중에 받을수록 가치가 줄어든다.
이를 수학적으로 표현한 개념이 "감가율(Discount Factor)"

감가율 : 시간에 따라서 감가하는 비율    γ ∈ [0, 1]

현재의 시간 t로부터 시간 k가 지난 후에 받는 보상이 R_t+k라면
현재 그 보상의 가치는 y^k-1R_t+k와 같다.
즉, 더 먼 미래에 받을 수록 에이전트가 받는 보상의 크기는 줄어든다.


## 정책

모든 상태에서 에이전트가 할 행동

ㆍ 상태가 입력으로 들어오면 행동을 출력으로 내보내는 일종의 함수
ㆍ 하나의 행동만을 나타낼 수도 있고, 확률적으로 a1 = 10%, a2 = 90%로 나타낼 수도 있다.
    π(a|s) = P[A_t = a|S_t = s]
ㆍ 시간 t에 에이전트가 S_t = s에 있을 때 가능한 행동 중에서 A_t = a를 할 확률
ㆍ 강화 학습 문제를 통해 알고 싶은 것은 정책이 아닌 "최적 정책"


## 가치함수
우리가 지금까지 한 일 : 문제를 MDP로 정의
→ 에이전트는 MDP를 통해 최적 정책을 찾으면 된다.

하지만 에이전트가 어떻게 최적 정책을 찾을 수 있을까?

에이전트 입장에서 어떤 행동을 하는 것이 좋은지를 어떻게 알 수 있을까?
→ 현재 상태에서 앞으로 받을 보상을 고려해서 선택해야 좋은 선택 !
하지만 아직 받지 않은 보상들을 어떻게 고려할 수 있을까?
→ 에이전트는 가치함수를 통해 행동을 선택할 수 있다.

[ MDP → 가치함수 → 행동 선택 ]

현재 시간 t부터 에이전트가 행동을 하면서 받을 보상을 모두 더해보자.
    R_t+1 + R_t+2 + R_t+3 + R_t+4 + R_t+5 + ㆍㆍㆍ
하지만 이 수식에는 3가지 문제가 있다.

1. 현재에 받은 보상 100과 미래에 받는 보상 100을 똑같이 취급한다.
2. 보상 100을 1번 받을 때와 보상 20을 5번 받을 떄를 구분하지 못한다.
3. 시간이 무한대라면 0.1을 받아도, 1을 받아도 합이 무한대다.

단순한 보상의 합으로는 판단을 내리기 어려우므로, 좀 더 정확한 판단을 위해 감가율을 고려한다.

R_t+1 + R_t+2 + R_t+3 + R_t+4 + R_t+5 + ㆍㆍㆍ
                    ↓
R_t+1 + γR_t+2 + γ^2R_t+3 + γ^3R_t+4 + γ^4R_t+5 + ㆍㆍㆍ

이 값을 반환값(Return) G_t라고 한다.
예를 들어, 에피소드를 t = 1부터 5까지 진행했다면
G1 = R2 + γR3 + γ^2R4 + γ^3R5 + γ^4R6
G2 = R3 + γR4 + γ^2R5 + γ^3R6
G3 = R4 + γR5 + γ2R6
G4 = R5 + γR6
G5 = R6

에이전트는 에피소드가 끝난 후에야 반환값을 알 수 있다.
하지만 에피소드가 끝날 때까지 기다려야 할까?
때로는 정확한 값을 얻기 위해 끝까지 기다리는 것보다
정확하지 않더라도 현재의 정보를 토대로 행동하는 것이 나을 때가 있다.
→ 가치함수 = 어떠한 상태에 가면 받을 것이라고 예상되는 값
    v(s) = E[G_t | S_t = s]
                ↓
    v(s) = E[R_t+1 + γRt+2 + γ^2R_t+3 + ㆍㆍㆍ|S_t = s]
    v(s) = E[R_t+1 + γ(R_t+2 + γR_t+3 + ㆍㆍㆍ)|S_t = s]
    v(s) = E[R_t+1 + γG_t+1|S_t = s]

ㆍ 반환값으로 나타내는 가치함수
    v(s) = E[R_t+1 + γG_t+1|S_t = s]
ㆍ 가치함수로 표현하는 가치함수의 정의
    v(s) = E[R_t+1 + γv(S_t+1)|S_t = s]
여기까지는 가치함수를 정의할 떄 정책을 고려하지 않음.

하지만 정책을 고려하지 않으면 안된다.
ㆍ 상태에서 상태로 넘어갈 때 에이전트는 무조건 행동을 해야하고
   각 상태에서 행동을 하는 것이 에이전트의 정책이기 때문
ㆍ 정책에 따라서 계산하는 가치함수는 당연히 달라질 수밖에 없음
ㆍ MDP에서의 가치함수는 항상 정책을 고려


벨만 기대 방정식(Bellman Expectation Equation)
    vπ(s) = Eπ[R_t+1 + γvπ(S_t+1)|S_t = s]
ㆍ 현재 상태의 가치 함수와 다음 상태의 가치함수 사이의 관계를 말해주는 방정식
ㆍ 강화학습은 벨만 방정식을 어떻게 풀어나가느냐의 스토리



## 큐함수
가치함수는 말 그대로 "함수" → 입력/출력이 무엇인지 알아야 한다.

[ 상태 → 가치함수 → 받을 보상의 합]
  입력                   출력

ㆍ 지금까지 설명한 가치함수는 상태 가치함수
ㆍ 에이전트는 가치함수를 통해 다음에 어떤 상태로 가야할 지 판단할 수 있다.
ㆍ 어떤 상태로 가면 좋을지 판단한 후에 그 상태로 가기 위한 행동을 따져볼 것이다.

하지만...

ㆍ 어떤 상태에서 각 행동에 대해 따로 가치함수를 만들어서 그 정보를 얻어올 수 있다면
   에이전트는 굳이 다음 상태의 가치 함수를 따져보지 않아도 어떤 행동을 해야할 지 선택할 수 있다.
ㆍ 이처럼 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려주는 함수를 행동 가치함수라고 한다.
   → 큐함수 (Q Function)

가치함수와 큐함수 사이의 관계
vπ(s) = Σ π(a|s)qπ(s,a)
       a∈A

1. 각 행동을 했을 때 앞으로 받을 보상인 큐함수 qπ(s,a)를 π(a|s)에 곱한다.
2. 모든 행동에 대해 큐함수와 정책을 곱한 값을 더하면 가치함수가 된다.

큐함수는 강화학습에서 중요한 역할을 한다.

ㆍ 강화학습에서 에이전트가 행동을 선택하는 기준으로 가치함수보다는 보통 큐함수를 사용한다.

큐함수 또한 벨만 기대 방정식의 형태로 나타낼 수 있다.
    qπ(s,a) = Eπ[R_t+1 + γqπ(S_t+1,A_t+1)|S_t=s, A_t= a]
ㆍ 가치함수의 식과 다른 점은 조건문에 행동이 더 들어간다는 점


## 벨만 기대 방정식
    vπ(s) = Eπ[R_t+1 + γvπ(S_t+1)|S_t = s]
위 함수는 현재 가치함수 값을 갱신한다.
하지만 갱신하려면 기댓값을 계산해야 하는데 어떻게 계산할까?
ㆍ 기댓값에는 어떤 행동을 할 확률(정책π(a|s))과
   그 행동을 했을 때 어떤 상태로 가게 되는 확률 (상태 변환 확률 P_ss'^a)이 포함되어 있다.
ㆍ 따라서 정책과 상태 변환 확률을 포함해서 계산하면 된다.

    vπ(s) = Σ π(a|s)(R_t+1 + γΣPss'^aㆍv_π(s'))
           a∈A              s'∈S

상태 변환 확률을 모든 s와 a에 대해 1이라고 가정하자.
그리고 다음 예제를 한 번 생각해보자.

ㆍ 행동은 "상, 하, 좌, 우" 4가지
ㆍ 에이전트의 초기 정책은 무작위로 각 행동의 선택 확률은 25%
ㆍ 현재 에이전트 상태에 저장된 가치함수는 0
    ㆍ 왼쪽 상태의 가치함수는 1, 밑쪽 상태의 가치함수는 0.5
       위쪽 상태의 가치함수는 0, 오른쪽 상태의 가치함수는 0
ㆍ 감가율은 0.9
ㆍ 오른쪽으로 행동을 취할 경우
   노란색 별로 표현된 1의 보상을 받음

vπ(s) = Σ π(a|s)(R_t+1 + γvπ(s'))
       a∈A

ㆍ 행동 = 상 : 0.25 x (0 + 0.9 x 0) = 0
ㆍ 행동 = 하 : 0.25 x (0 + 0.9 x 0.5) = 0.1125
ㆍ 행동 = 좌 : 0.25 x (0 + 0.9 x 1) = 0.225
ㆍ 행동 = 우 : 0.25 x (1 + 0.9 x 0) = 0.25
ㆍ 기댓값 = 0 + 0.1125 + 0.225 + 0.25 = 0.5875


## 벨만 최적 방정식
    vπ(s) = Eπ[R_t+1 + γvπ(S_t+1)|S_t = s]
벨만 기대 방정식을 통해 계속 계산을 진행하다 보면
언젠가 식의 왼쪽 항과 오른쪽 항이 동일해지는 순간이 온다.
→ vπ(s) 값이 수렴 → 현재 정책 π에 대한 참 가치함수를 구한 것

하지만 참 가치함수와 최적 가치함수는 다르다.
ㆍ 참 가치함수는 "어떤 정책"을 따라서 움직였을 경우에 받게 되는 보상에 대한 참값
ㆍ 가치함수의 정의가 "현재로부터 미래까지 받을 보상의 총합"인데
   이 값이 얼마나 될지에 대한 값
ㆍ 하지만 최적의 가치함수는 수많은 정책 중에서 가장 높은 보상을 주는 가치함수다.
    v_k+1(s) = π(a|s)(R^a_s + γv_k(s'))
ㆍ v_k+1(s) : 현재 정책에 따라 k + 1번째 계산한 가치함수 (그 중에서 상태 s의 가치함수)
ㆍ k + 1번째의 가치함수는 k번째 가치함수 중에서 주변 상태들 s'을 이용해 구한다.
ㆍ 그리고 이 계산은 모든 상태에 대해 동시에 진행한다.

강화학습은 MDP로 정의되는 문제에서 최적 정책을 찾는다.
더 좋은 정책은 무엇일까? 어떤 정책이 더 좋은 정책일까?

→ 가치함수(받을 보상들의 합)를 통해 판단할 수 있다.
모든 정책에 대해 가장 큰 가치함수를 주는 정책이 최적 정책
→ 최적 정책을 따라갔을 때 받을 보상의 합이 최적 가치함수
    v_*(s) = max(v_π(s))
              π
    q_*(s,a) = max[q_π(s,a)]
                π

최적 정책은 각 상태 s에서의 최적의 큐함수 중에서
가장 큰 큐함수를 가진 행동을 하는 것
→ 선택 상황에서 판단 기준은 큐함수이며, 최적 정책은 언제나 이 큐함수 중에서
가장 높은 행동을 하나 하는 것

            1,      if a = argmasx q_*(s,a)
π_*(s,a) = {             a∈A
            0,      otherwise


최적의 가치함수 = 최적의 큐함수 중에서 최대를 선택하는 것
    v_*(s) = max[q_*(s,a) | S_t = s,A_t = a]
              a
큐함수를 가치함수로 고쳐보자.
    v_*(s) = maxE[R_t+1 + γv_*(S_t+1) | S_t = s, A_t = a]
이를 벨만 최적 방정식(Bellman Optimality Equation)이라고 한다.
    q_*(s,a) = E[R_t+1 + γmaxq_*(S_t+1, a') | S_t = s, A_t = a]
                           a



## 다이내믹 프로그래밍
한번에 풀기 어려운 문제를 여러 개의 작은 문제로 나눠 푸는 것
ㆍ 작은 문제들 사이에서 공유하는 계산 결과들을 재사용해 총 계산량을 줄일 수 있음
ㆍ 예) 총 4칸인 계단을 한 번에 1칸, 2칸씩 오르는 경우의 수는?

    v_π(s) = E_π[R_t+1 + γR_t+2 + γ^2R_t+3 + ㆍㆍㆍ | S_t = s]

가치함수(v_π(s))를 기준으로 정책이 얼마나 좋은 지 평가한다.
문제는 정책을 평가하려면 v_π(s)가 필요한데 이게 뭔 지 모른다.
→ 다이나믹 프로그래밍으로 해결